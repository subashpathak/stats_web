<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Residuals</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Residuals</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Jun 2018 00:00:00 +0100</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Residuals</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big picture behind P-values</title>
      <link>/post/pvalues-big-picture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/pvalues-big-picture/</guid>
      <description>


&lt;p&gt;About a year ago, ASA (American Statistical Association) published a series of articles urging journals and research investigators to ban the use of pvalues in their findings. You might be thinking, what is wrong with p-values? The answer is: there is nothing inherently wrong with pvalues. Its the way they are interpreted.&lt;/p&gt;
&lt;p&gt;This post shows you overall picture behind p-value. Lets assume you are conducting an experiment (it could be testing the difference in efficacy between 2 drugs in medicine which we call clinical trial or testing difference between two product strategies in business settings commonly known as A/B testing.) Lets say after the experiment is over, you calculated pvalue of 0.02. What does this 0.02 pvalue telling us?&lt;/p&gt;
&lt;p&gt;We conduct an experiment with the assumption that there is no difference in between groups we are testing which is called null hypothesis. The idea is to see how far off we are from our null hypothesis (i.e the assumption of equivalent group) after we finished the experiment. P-value is built on this idea of assumption that there is no difference between groups. In our case pvalue is 0.02 which says that, provided this assumption of no difference between groups, if we repeateadly conduct 1000 similar experiments, only 20 such experiments would give us results as or more surprising than our current result. This tells you that since very few number of repeated experiments would give us the results as or more surprising than our current data, we have some degree of evidence against our assumption of equivalent groups. However, this doesn’t mean alternative is 100% true.&lt;/p&gt;
&lt;p&gt;Above p-value of 0.02 might seem impressive for any experiment,and people tend to conclude there is a “statistically significant difference” between 2 groups and use it as the ultimate truth. So, the point is rather than accepting p-value as the ultimate truth, we need to look at other measures of uncertainty surrounding it. As an investigator one needs to look at the magnitude of difference between 2 groups and confidence intervel associated with it and see if it is meaningful enough. Uncertainty expressed as the difference between groups (which we call “effect size”) and confidence interval help explore research findings in a deeper level. Blindly trusting results based on p-value cutoff won’t lead to good research.&lt;/p&gt;
&lt;p&gt;(I have explained confidence intervel in this post)[&lt;a href=&#34;https://residuals.netlify.com/concepts/confidence-interval/&#34; class=&#34;uri&#34;&gt;https://residuals.netlify.com/concepts/confidence-interval/&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;I will talk more about misconceptions of pvalues and how we can calculate p-values by hand in later posts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Central Limit Theorem</title>
      <link>/post/central-limit-theorem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/central-limit-theorem/</guid>
      <description>


&lt;div id=&#34;what-is-central-limit-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is Central Limit Theorem?&lt;/h2&gt;
&lt;p&gt;Textbook definition of Central Limit Theorem states that “As sample size increases, &lt;a href=&#34;https://statsisfun.netlify.com/concepts/sampling-dist-sample-mean/&#34;&gt;the sampling distribution of the mean&lt;/a&gt; for an independent identically distributed variable will approximately resemble that of normal distribution regardless of its distribution”.&lt;/p&gt;
&lt;p&gt;This definition of CLT needs some breakdown into simpler parts. First of all, a random variable can have different probability distribution. It could be right, left skewed or uniformly distributed etc.. However, if that variable has sufficient enough sample size, its distribution approximates normal distribution irrespective of the probability distribution. We will explore this key idea using a series of simulation examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#set libraries
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------------------------- tidyverse 1.2.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.1.0       v purrr   0.3.1  
## v tibble  2.0.1       v dplyr   0.8.0.1
## v tidyr   0.8.3       v stringr 1.4.0  
## v readr   1.3.1       v forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s look at a right skewed (negative binomial distribution)
set.seed(1536)
#this gives us a set of random numbers that follow negative binomial distribution with a sample size of 100
neg_binom&amp;lt;-rnbinom(100,30,0.3)
neg_binom1&amp;lt;-as.data.frame(neg_binom)

#plot histogram and density curve
hist&amp;lt;- ggplot(neg_binom1, aes(x=neg_binom)) + 
  geom_histogram(aes(y=..density..),bins=20) + geom_density() + 
  labs(x=&amp;quot;X&amp;quot;,y=&amp;quot;Density&amp;quot;) + ggtitle(&amp;quot;Right Skewed negative binomial distribution&amp;quot;)
hist&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/central-limit-theorem_files/figure-html/central-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we increase sample size sufficiently enough, we will see the distibution of the variable X resembles to that of normal distribution. This means we will acheive a symmetrical distribution of this right skewed variable if we have larger enough sample size. Lets see if we can show this in an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s take the same negative binomial distribution with large sample size 

set.seed(1546)
#this gives us a set of random numbers that follow negative binomial distribution with a sample size of 10000
neg_binom&amp;lt;-rnbinom(10000,30,0.3)
neg_binom1&amp;lt;-as.data.frame(neg_binom)

#plot histogram and density curve
hist&amp;lt;- ggplot(neg_binom1, aes(x=neg_binom)) + 
  geom_histogram(aes(y=..density..),bins=20) + geom_density() + 
  labs(x=&amp;quot;X&amp;quot;,y=&amp;quot;Density&amp;quot;) + ggtitle(&amp;quot;Normal Distribution obtained using larger sample size&amp;quot;)
hist&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/central-limit-theorem_files/figure-html/central1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, once we sufficiently increase the number of samples, the right skewed binomial distribution with same parameters resembles normal distribution. As we can clearly see from the figure that concentration of the values generated is around the center which is mean of the distribution approximating normal distribution.&lt;/p&gt;
&lt;p&gt;You can take any skewed family of distribution and repeat above procedures of simulation with larger sample size and you will get the distribution that approximates normal distribution. This concept of central limit theorem has broader implications in the field of statistical inferences. I will expand on this in more details in future posts.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Confidence Interval</title>
      <link>/post/confidence-interval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/confidence-interval/</guid>
      <description>


&lt;div id=&#34;how-do-you-explain-confidence-intervals-intuitively&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How do you explain confidence intervals intuitively?&lt;/h2&gt;
&lt;p&gt;In any study,We all know we want to test our hypothesis to see if there is any difference between our assigned groups using some kind of experiment. As a common rule, point estimate is calculated as the difference in means between two groups and 95 % confidence interval is reported as a means of quantifying uncertainty associated with that estimate.&lt;/p&gt;
&lt;p&gt;When people hear about 95% confidence interval, they mistakenly interpret it as the interval with 95% chance of containing true population parameter we are trying to estimate. This is entirely a wrong way to think about confidence interval. Let me make this clear with a simulation example.&lt;/p&gt;
&lt;p&gt;Textbook definition of 95% confidence interval tells us that if we repeatedly conduct experiments of sample size n and calculate confidence interval in each case, 95 % of those intervals would contain the population parameter we are trying to estimate.&lt;/p&gt;
&lt;p&gt;Assume an experiment where we want to find out the effect of a treatment (Active vs Control) on our outcome. Lets assume for our outcome simulation We have following population parameters. It means we know true population effect here since we created this for simulation. It is always unknown in real world.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;true population effect delta= 200 (true difference in mean outcome between Active and control group)&lt;/li&gt;
&lt;li&gt;standard deviation = 100&lt;/li&gt;
&lt;li&gt;Sample size = 200&lt;/li&gt;
&lt;li&gt;Number of trials repeated with this sample size = 100&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(148)
n &amp;lt;- 200
sigma&amp;lt;-100
delta&amp;lt;-200
# Store means, upper and lower intervals from 100 trials
means&amp;lt;-c()
upper&amp;lt;-c()
lower&amp;lt;-c()

for (i in 1:100) {
  #Simulate a normal distribution using above parameters
  x&amp;lt;-rnorm(n,delta,sigma)
  means[i]&amp;lt;-mean(x)
  lower[i]&amp;lt;-means[i]-1.96*sigma/sqrt(n)
  upper[i]&amp;lt;- means[i]+1.96*sigma/sqrt(n)
  
}

# change to dataframe
means_frame&amp;lt;- as.data.frame(means)
id&amp;lt;-rownames(means_frame)
id&amp;lt;-as.numeric(id)
means_list&amp;lt;-cbind(means_frame,id)

upp_lower&amp;lt;-as.data.frame(cbind(lower,upper))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot 95% confidence interval for each estimate
 true_value&amp;lt;-ifelse (lower &amp;gt;= 200 &amp;amp; upper &amp;gt;= 200 ,1,0) 
true&amp;lt;-as.data.frame(true_value)

#Combine everything into one dataset

all&amp;lt;-cbind(upp_lower,true, means_list)

g&amp;lt;-ggplot(all,aes(id,means,color=factor(true_value,labels=c(&amp;quot;Includes true effect&amp;quot;,&amp;quot;Excludes true effect&amp;quot;)))) +
  geom_errorbar(aes(ymin=lower,ymax=upper)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(limits=c(0,100)) +
  labs(x=&amp;quot;Number of Trials&amp;quot;,y=&amp;quot;Confidence limit&amp;quot;,color=&amp;quot;Confidence&amp;quot;) +
  geom_hline(yintercept=200,color=&amp;quot;black&amp;quot;,linetype=&amp;quot;dashed&amp;quot;,size=0.5)

g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/confidence-interval_files/figure-html/plots-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, as stated in the definition of 95% confidence interval we calculated confidence intervals for 100 trials with same sample size. As expected we clearly see from above figure (indicated by green dots and error bars), 5 trials out of 100 didn’t have confidence limit that contain true population effect of 200. Remaining 95 intervals contain true population effect.&lt;/p&gt;
&lt;p&gt;It is important to notice that we will never know true population effect in practice. Some intervals will contain true value whereas some won’t. We cannot tell which contain the true effect. However, provided that we know nothing about true effect, we can assign a probability for the confidence interval being included in our analysis. Looking at the figure above we can say 95 out of 100 i.e. 95% of the intervals cover true value.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inappropriate use of Bivariable analysis to screen risk factors for use in Multivariable analysis</title>
      <link>/paper/bvs_method_paper_summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/paper/bvs_method_paper_summary/</guid>
      <description>


&lt;div id=&#34;link-to-the-paper&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Link to the paper&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/8699212&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/8699212&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper goes into in-depth discussion of how selecting variables based on significance achieved using pvalue criterion of less than 0.05 for use in multivariable analysis won’t be able to embrace the confounders sufficient to control for confounding. When we are only looking at bivariable analysis, it only gives us unadjusted association between one single risk factor and our outcome of interest. This won’t tell us anything about intercorrelation or mutual confounding among risk factors. If risk factors (independent variables) are not truly independent of each other, a nonsignificant risk factor in bivariable analysis is not necessarily nonsignificant in multivariable analysis. BVS (Bivariable selection method) could potentially prevent important variable from being included in multivariable model which could lead to distorted or incomplete findings.&lt;/p&gt;
&lt;p&gt;Authors in the paper have shown some hypothetical as well practical examples where they have clearly indicated how BVS method could be problematic when creating multivariable model. In all these examples they have shown how one risk factor could be mutually confounded to other risk factors. If we use BVS method, there is no way we would be able to know mutual confounding among risk factors and this would lead to incorrect or imprecise formulation of multivariable model.&lt;/p&gt;
&lt;p&gt;Some suggestions and alternatives to BVS method:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Investigators should think carefully about the candidate variables that could affect their outcome of interest and use them all in MV model if its only a small set of variables.&lt;/li&gt;
&lt;li&gt;If collinearity exists due to the inclusion of large set of independent variables and the investigator wishes to screen variables in order to avoid collinearity, this type of screening must be based on prior knowledge of the variables or principles of subject matter being studied; alternatively, other special statistical techniques like regularized regression or principal component analysis can be used. The point is that investigator should not let a computer program decide how to handle collinearity. Additionally, BVS method is useless to solve any problem caused by collinearity.&lt;/li&gt;
&lt;li&gt;Only those variables that are known or expected to be risk factors for the outcome by either principles of the study or the prior knowledge of the variables should be included in MV model.&lt;/li&gt;
&lt;li&gt;Independent variables with empty cells in contingency table with discrete outcome can be handled by collapsing the categories or treating the variable as continuous.&lt;/li&gt;
&lt;li&gt;(My suggestion): I think it is important for investigators to think about cause-effect relationship between outcome and risk factors drawing Directed Acyclic Graphs (DAGs). This will bring clarity in the research question being explored and help formulate multivariable model that would reduce confounding and bias in our analysis. (“The book of why: The new science of Cause and effect” by Judea Pearl is an excellent resource to learn about DAGs)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In conclusion, authors point out that testing bivariable association between variables in the data cannot demonstrate whether a variable is a confounder regardless of the types of statistical methods being used. When a confounder exists and is not properly controlled, estimation of the effect of risk factor on outcome is biased and distorted. To sum up, this paper illustrates BVS method is not able to correct for possible confounders, and its use is not an appropriate way to select variables to be used in multivariable analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Notes and Summary of the paper Seven Myths of Randomization</title>
      <link>/paper/seven_myths_of_randomisation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/paper/seven_myths_of_randomisation/</guid>
      <description>


&lt;div id=&#34;seven-myths-of-randomization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Seven Myths of Randomization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Link to the paper&lt;/strong&gt; &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.5713&#34; class=&#34;uri&#34;&gt;https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.5713&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Link to the powerpoint presentation&lt;/strong&gt; &lt;a href=&#34;https://www.methodologyhubs.mrc.ac.uk/files/9214/3711/9501/Plenary-_Stephen_Senn.pdf&#34; class=&#34;uri&#34;&gt;https://www.methodologyhubs.mrc.ac.uk/files/9214/3711/9501/Plenary-_Stephen_Senn.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This paper offers some valuable insights into some of the myths of randomisation that seem to be popular among researchers and investigators.Some concerns are related to the practical realities of clinical research on patients, some regarding balance and some regarding the role of conditioning in a valid statistical inference.&lt;/p&gt;
&lt;p&gt;Author makes use of an interesting example of game of rolling red and black dice to illustrate his points. Lets assume we want to know the probability of score of sum 10 from the rolling of red and black die. He goes on to list three variants of the game: 1. Variant 1: First state the probaility of 10 and roll the two dice together. The answer would be 4/36=1/12 2. Variant 2: The red die is rolled so that statistician knows the result and he rolls black die afterwards. probability of sum of 10 here would be (1/2* 0)+ (1/2*1/6)=1/12 3. Variant 3: red die is rolled and the score is unknown to statistician. Then he calls the odds and black die is rolled afterwards. It would be equivalent to variant 1.&lt;/p&gt;
&lt;p&gt;The author says we can use these three scenarios as an analogy with clinical trial in which covariate information may or may not be available at baseline. In variant 1 there is no information available, in variant 2 there is info available at baseline and in variant 3 it might in principle, be available but nobody has seen it.&lt;/p&gt;
&lt;p&gt;Following are seven myths of Randomised clinical trials discussed in this paper:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Myth 1: Patients are treated simultaneously in clinical trials&lt;/strong&gt; This myth seems to be surprisingly persistent with critics of randomisation. Generally, patients are entered into clinical trial soon after they are present. The recruitment period might well be longer than the follow up period in which outcomes of the patients are observed. It could be the case that some patients might have completed their trial before others have even started.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Myth 2: Balance of prognostic factors is necessary for valid inference&lt;/strong&gt; Balance of prognostic factors or covariates in a clinical trial may not happen all the time. That doesn’t mean our trial cannot be used to generate statistical inference. The imbalance seen in covariates are only by chance and we cannot think of it as a flawed study. If there is an imbalance in favor of treatment A, one can always find another imbalance that is equally in favor of treatment B if sufficiently many additional baseline characteristics are examined. Thus the imbalances cancel anyway.This source &lt;a href=&#34;https://discourse.datamethods.org/t/should-we-ignore-covariate-imbalance-and-stop-presenting-a-stratified-table-one-for-randomized-trials/547/3&#34;&gt;here&lt;/a&gt; by Professor Frank Harrell provides clear description with simulations on why we should not worry about covariate imbalance in randomised clinical trials.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Myth 3: Blinding can be carried out effectively without randomisation&lt;/strong&gt; Some critics believe that blinding can be carried out effectively without randomisation. In fact randomisation assures maximum unguessability of any sequence of allocation of treatment. We can’t do this effectively with random allocation of patients to different treatments. By definition, randomisation ensures each and every participant has equal and independent probablilty of being allocated to any treatment group in the study which implies the need for strong blinding. If you don’t randomise you have to assume that your strategy has not been guessed by the investigator.Not publishing the block size in your protocol is a classic example.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Myth 4: Randomisation is insufficient&lt;/strong&gt; This is in some sense is not a myth. Randomisation is not fully efficient and there seems to be loss of one patient per factor fitted compared to a completely balanced design which is not usually possible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Myth 5: Randomisation precludes balancing covariates&lt;/strong&gt; We can build strata and randomise within them. Fisher’s strategy was balance what you can and randomise what you can’t.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Myth 6: Observed covariates may be ignored because one has randomised&lt;/strong&gt; This is the myth even some statisticians seem to believe. To ignore observed prognostics is to treat variant 2 of the game as if it were game 1. In case of game 1, it is necessary to consider with what probability each of the six scores of red die could arrive if it cannot be observed beforehand. However, once we know the score of the red die, this probability is no longer relevant. Also, conditioning on pre-specified prognostic factor seems to increase precision of our estimate increasing statistical power and reducing sample size requirements &lt;a href=&#34;https://www.jclinepi.com/article/S0895-4356(03)00379-2/fulltext&#34;&gt;Paper Link on this topic&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Myth 7: Large Trials are more balanced than small ones&lt;/strong&gt; Large and small trials are equally balanced. Its just that with larger trials we have narrower confidence limit and even small effect can be observed resulting in small p-values. This advantage of increased mean balance in covariates has been consumed in the form of narrower limits.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;For more details on links and lists of things regarding common statistical myth, following link can be used:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference collection to pushback against common statistical myths&lt;/strong&gt; &lt;a href=&#34;https://discourse.datamethods.org/t/reference-collection-to-push-back-against-common-statistical-myths/1787&#34; class=&#34;uri&#34;&gt;https://discourse.datamethods.org/t/reference-collection-to-push-back-against-common-statistical-myths/1787&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Notes and Summary of the paper The primary outcome is positive- Is that Good Enough ?</title>
      <link>/paper/primary-outcome-positive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/paper/primary-outcome-positive/</guid>
      <description>


&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Link to the paper&lt;/strong&gt; &lt;a href=&#34;https://www.nejm.org/doi/full/10.1056/NEJMra1601511&#34; class=&#34;uri&#34;&gt;https://www.nejm.org/doi/full/10.1056/NEJMra1601511&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is often the case that people tend to simplify positive results from a trial as a binary conclusion. However, positive results from primary findings do not guarantee everything done during the course of study was successful and well planned. The authors have presented their arguments saying that determination whether findings provide evidence that is sufficient to modify medical practice requires in depth interpretation of the trial data and the results of earlier related trials. This paper lays out some important questions regarding what we need to think about even if we see positive primary outcome. Usually, the acheivement of statistical signficance for the primary outcome seems to be a prerequisite for the adoption of a new therapy, but it is not sufficient. Authors in this review article have laid out following questions to ask when you see positive primary outcome in your study:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Does a P value of &amp;lt;0.05 provide strong enough evidence?&lt;/li&gt;
&lt;li&gt;What is the magnitude of the treatment benefit?&lt;/li&gt;
&lt;li&gt;Is the primary outcome clinically important (and internally consistent)?&lt;/li&gt;
&lt;li&gt;Are secondary outcomes supportive?&lt;/li&gt;
&lt;li&gt;Are the principal findings consistent across important subgroups?&lt;/li&gt;
&lt;li&gt;Is the trial large enough to be convincing?&lt;/li&gt;
&lt;li&gt;Was the trial stopped early?&lt;/li&gt;
&lt;li&gt;Do concerns about safety counterbalance positive efficacy?&lt;/li&gt;
&lt;li&gt;Is the efficacy-safety balance patient-specific?&lt;/li&gt;
&lt;li&gt;Are there flaws in trial design and conduct?&lt;/li&gt;
&lt;li&gt;Do the findings apply to my patients?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will be summarizing and reviewing these questions based on how they are presented in this paper.&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;Does a P value of &amp;lt;0.05 provide strong enough evidence ?&lt;/strong&gt; According to the authors, if we are looking for strong enough evidence of efficay in a trial based on p-value criterian of &amp;lt;0.05, smaller pvalue is always better.They argue PARADIAM-HF trial of sacubitrilvalsartan versus enalapril in patients with heart failure showed overwhelming benefit (p&amp;lt;0.00001) with respect to the composite primary outcome cardiovascular death or hospitalization for heart failure which justified in regulatory approval and clinical adoption of this drug. In contrast to this,NXY-059 of SAINT I trial compared with placebo patients for the treatment of ischemic stroke (primary outcome: disability at 90 days) did not provide strong evidence of efficiency with a small pvalue of 0.038. This caused them to conduct a second SAINT II trial that revealed no signficant effect (p value of 0.33). I am assuming when approving or rejecting the adoption of these drugs they also looked at other metrics of drug evaluation like how big is the effect size (clinically meaningful), confidence intervals etc.. besides p-values.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;What is the magnitude of the treatment effect&lt;/strong&gt; Treatment effect needs to be clinically meaningful (large enough to matter) beyond statistical significance and their determination requires examination of treatment effect on both a relative scale (eg. by calculation of relative risk or hazard ratio) and an absolute scale (eg. by calculation of the differences in the rates of events during follow-up and in the number needed to treat). The extent of uncertainty associated with the effect size should be considered by examining 95% confidence interval. For example, in IMPROVE-IT trial, for ezetimibe compared with placebo in patients with acute coronary syndromes who were being treated with simvastatin, the hazard ratio for the composite primary outcome of cardiovascular death,myocardial infarction, unstable angina, revascularization, or stroke was 0.94 (95% confidence Interval [CI], 0.89 to .98; p=0.016). The 7-year primary event rates were 32.7% with ezetimibe versus 34.7% with placebo (difference of 2% points, CI=around 0 to 4% points). Although findings of the trial were described as positive,small effect size caused to question whether the benefit of ezetimibe is large enough to warrant its cost and potential implications.An advisory panel from FDA recommended against expanding the ezetimibe label to including an indication for a reduction in cardiovascular events.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Is the primary outcome clinically important (and internally consistent)?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Surrogate points Phase 3 trials are usually powered to achieve clinically relevant, but some diseases use of surrogate primary outcome measure has been accepted. (eg. a reduction in glycated hemoglobin levels as an indication of antiglycemic efficacy in patients with diabetes).Some large scale trials have raised questions regarding the wisdom of such reliance on surrogate markers. In ACCORD trial, intensive therapy resulted in markedly lower glycated hemoglobin levels than standard therapy, but the rate of cardiovascular events was not significantly lower, and mortality was higher. Similarly, in LIDO trial, the approval of the drug levosimendan which resulted in greater hemodynamic improvement (the primary surrogate outcome) than dobutamine patients with acute heart failure was not accepted by FDA. The reason was, SURVIVE, a larger , subsequent trial of levosimendan vs. dobutamine showed no evidence of a treatment benefit for the primary outcome (180 day mortality).&lt;/li&gt;
&lt;li&gt;Composite outcomes Positive composite outcomes must be carefully inspected to determine which components are driving the result. As an example, in RITA trial,fewer patients in intervention group had composite primary outcome of death,myocardial infarcation or refractory angina than compared to conservative group (9.6% vs. 14.5%, p=0.001)&amp;gt; In fact, the result in the difference was driven by halving of the rate of refractory angina, with no evidence of difference in deaths or myocardial infarction at short term.. Fortunately, a 5 year follow up study showed a 22% lower risk of death or myocardial infarcation with the use of interventional approach vs. conservative approach. This lead to the supported use of early interventional approach in patients with accute coronary syndromes to improve diagnosis. In short,When analysing composite outcome we need to parse out which component seems to be driving majority of the outcome results and approach the analysis accordingly.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Are seconadry outcomes supportive&lt;/strong&gt; Confidence in the overall positive results of primary outcome is enhanced if prespecified secondary outcomes also show treatment benefit. For example, people had doubts about positive primary outcome in SAINT I trial of NXY-059 in acute ischemic stroke as prespecified secondary outcomes- scores on the National institutes of Health Stroke Scale and the Barthel Index showed no evidence of benefit between two treatment groups. Secondary outcomes can be helpful in making sure positive primary outcome results are strong enough to believe.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Are findings consistent across subgroups?&lt;/strong&gt; Relative treatment effects may vary according to patient characteristics. Also,consistent relative treatment effect may be observed across all patient types except for certaing high risk subgroups who may have greater absolute benefits. Sometimes, subgroup analysis identify patients who do not appear to benefit from new treatment despite the primary findings being positive. Subgroup analysis could be showing spurious findings only. However, protecting such patients from ineffective treatment should be considered depending on the strength of statistical interaction and its biologic plausability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Is the trial large enough to be convincing?&lt;/strong&gt; We need to be cautious about being confident on the positive primary findings obtained from small trials. Due to the lack of enough sample size, small trials lack power to detect the true signals; so, positive treatment effects are susceptible to exaggeration and false positives.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Was the trial stopped early&lt;/strong&gt; Sometimes a trial is stopped early because interim results show strong evidence of treatment superiority, which is often a newsworthy event. Unfortunately, this practice tends to exaggerate treatment efficacy. As trial progresses, the estimated treatment effect varies randomly in relation to the true effect. If the interim estimate is based on randomly high indication of efficacy, it is more likely to cross a stopping boundary and to convince a data and safety monitoring board that overwhelming evidence of benefit exists. Stopping early also truncates eveidence for important secondary (and safety) outcomes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Do concerns about safety counterbalance positive efficacy?&lt;/strong&gt; When a new treatment has superior efficacy, it is important to identify concerns about safety that might offset the benefits. A balanced account of both efficacy and safety must be provided. Absolute benefits and risks should be presented in terms of differences and percentages. Consideration of number needed to treat for benefit vs. the number needed to harm may provide a guide to net clinical benefit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Is the balance of efficacy and safety patient specific?&lt;/strong&gt; The net clinical benefit of a new treatment may be patient-specific- that is, worthwhile for those at an increased risk for the primary efficacy outcome but deleterious for those at an increased risk of adverse events.Calculating the individual patient trade-offs between efficacy and safety is not straightforward, and statistical modeling techniques may be useful.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Are there flaws in trial design or conduct?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Sampling Distribution of the Sample Mean Clearly Explained</title>
      <link>/post/sampling-dist-sample-mean/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/sampling-dist-sample-mean/</guid>
      <description>


&lt;div id=&#34;what-is-sampling-distribution-of-the-sample-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is Sampling Distribution of the Sample Mean?&lt;/h2&gt;
&lt;p&gt;We use sample mean as one of the measures of summary of variables in a study. In other words, sample mean is the average of the values observed. Lets imagine you conduct the same study with same sample size again and again and calculate sample mean in each case. Frequency distribution of sample means obtained from those repeated studies is called sampling distribution of the mean. It can be displayed using histogram or density curve.&lt;/p&gt;
&lt;p&gt;For example, you run a study of sample size 12000 and replicate it 1000 times. Assume our data is normally distributed for simulation purposes. We will calculate 1000 sample means and show the distribution of those means using histogram overlayed with density curve as described below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#set libraries
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------------------------- tidyverse 1.2.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.1.0       v purrr   0.3.1  
## v tibble  2.0.1       v dplyr   0.8.0.1
## v tidyr   0.8.3       v stringr 1.4.0  
## v readr   1.3.1       v forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#replicate a study of sample size 12000  1000 times and calculate sample mean of each of those studies
set.seed(1431)
sample_avg = rep(NA, 1000) #replicate NA 1000 times
for (i in 1:1000){
  sample_avg[i]= mean(rnorm(12000))
}
#sample_avg&amp;lt;- as.integer(sample_avg)
#change the distribution of sample mean to data frame
sample&amp;lt;-as.data.frame(sample_avg)

#create histogram of sampling distribution of the sample mean overlayed with density curve
hist&amp;lt;- ggplot(sample, aes(x=sample_avg)) + 
  geom_histogram(aes(y=..density..)) + geom_density() + 
  labs(x=&amp;quot;Sample Average&amp;quot;,y=&amp;quot;Density&amp;quot;) + ggtitle(&amp;quot;Histogram showing sampling distribution of the sample mean&amp;quot;)
hist&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/sampling-dist-sample-mean_files/figure-html/simulation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This idea of sampling distribution is the fundamental concept based on which statistical inferences are made in frequentist Statistics. We will look at it more closely how it applies to one of the most important ideas in statistics called ‘Central Limit Theorem’ in another post.&lt;/p&gt;
&lt;p&gt;One thing that comes to mind after going through this explanation is that its not easy to repeat studies to estimate sampling distribution of sample mean in practice. We will have to use statistical techniques that would be able to estimate sampling distribution of the mean from a single random sample. We will discuss more about this in later posts in details.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
